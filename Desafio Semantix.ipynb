{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio Big Data Engineer - Semantix Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enviar os dados para o hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo cd spark/input\n",
    "\n",
    "sudo mkdir desafio\n",
    "\n",
    "sudo cd desafio\n",
    "\n",
    "sudo curl -O https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "sudo unrar x 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "sudo rm 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "docker exec -it namenode bash\n",
    "\n",
    "hdfs dfs -mkdir -p /user/eric/desafio\n",
    "\n",
    "hdfs dfs -put /input/desafio/* /user/eric/desafio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/eric/desafio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Database beeline \n",
    "\n",
    "docker exec -it hive-server bash\n",
    "\n",
    "beeline -u jdbc:hive2://localhost:10000\n",
    "\n",
    "create database COVID19 comment 'Desafio Semantix';\n",
    "\n",
    "SET hive.exec.dynamic.partition = true;\n",
    "\n",
    "SET hive.exec.dynamic.partition.mode = nonstrict ;\n",
    "\n",
    "create table covid19Brutos(\n",
    "    regiao string,\n",
    "    estado string,\n",
    "    municipio string,    \n",
    "    coduf int,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data string,\n",
    "    semanaEpi int,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado string,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitana string)\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    STORED AS TEXTFILE;\n",
    "\n",
    "LOAD DATA INPATH '/user/eric/desafio/*.csv' OVERWRITE INTO TABLE covid19Brutos;\n",
    "\n",
    "create table covid19(\n",
    "    regiao string,\n",
    "    estado string,    \n",
    "    coduf int,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data string,\n",
    "    semanaEpi int,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado string,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitana string)\n",
    "    partitioned by (municipio string);\n",
    "\t\n",
    "insert overwrite table covid19 partition (municipio) select * from covid19Brutos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3cfd14edd8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='covid19', description='', locationUri='hdfs://namenode:8020/user/hive/warehouse/covid19.db'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Database de trabalho\n",
    "spark.catalog.setCurrentDatabase(\"covid19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se esta conectado no database correto\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dadospainel_1', database='covid19', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listando as telas\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------------+----------------+----------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|      data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|incidenciaCasos|incidenciaObitos|letalidade|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------------+----------------+----------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|           0.00|            0.00|      null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: string (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: string (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: string (nullable = true)\n",
      " |-- incidenciaCasos: string (nullable = true)\n",
      " |-- incidenciaObitos: string (nullable = true)\n",
      " |-- letalidade: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2624943"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "estrutura_lista = [\n",
    "    \n",
    "    StructField(\"regiao\", StringType()),\n",
    "    StructField(\"estado\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"coduf\", StringType()),\n",
    "    StructField(\"codmun\", IntegerType()),\n",
    "    StructField(\"codRegiaoSaude\", IntegerType()),\n",
    "    StructField(\"nomeRegiaoSaude\", StringType()),\n",
    "    StructField(\"data\", StringType()),\n",
    "    StructField(\"semanaEpi\", IntegerType()),\n",
    "    StructField(\"populacaoTCU2019\", IntegerType()),\n",
    "    StructField(\"casosAcumulado\", IntegerType()),\n",
    "    StructField(\"casosNovos\", IntegerType()),\n",
    "    StructField(\"obitosAcumulado\", StringType()),\n",
    "    StructField(\"obitosNovos\", IntegerType()),\n",
    "    StructField(\"Recuperadosnovos\", IntegerType()),\n",
    "    StructField(\"emAcompanhamentoNovos\", IntegerType()),\n",
    "    StructField(\"interior/metropolitana\", StringType())\n",
    "]\n",
    "\n",
    "schema_names = StructType(estrutura_lista)\n",
    "\n",
    "dados = spark.read.csv(\"/user/eric/desafio/*.csv\", header='true',sep=';', schema = schema_names)\n",
    "dados = dados.withColumn(\"incidenciaCasos\", format_number(col(\"casosAcumulado\")/(col(\"populacaoTCU2019\")/100000),2))\n",
    "dados = dados.withColumn(\"incidenciaObitos\", format_number(col(\"obitosAcumulado\")/(col(\"populacaoTCU2019\")/100000),2))\n",
    "dados = dados.withColumn(\"letalidade\", format_number((col(\"obitosAcumulado\")/col(\"casosAcumulado\"))*100,2))\n",
    "\n",
    "dados.show(5)\n",
    "dados.printSchema()\n",
    "dados.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import timem\n",
    "ini = time.time()\n",
    "#dados.write.saveAsTable(\"covid19.DadosCovid19\",partitionBy=\"municipio\")\n",
    "dados.write.mode(\"overwrite\").partitionBy(\"municipio\").saveAsTable(\"covid19.DadosCovid\")\n",
    "fim = time.time()\n",
    "print (\"\\nTempo de Processamento: {0:4.2f} min\".format((fim-ini)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid19.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxData = dados.select(\"regiao\",\"data\").filter(col(\"regiao\") == \"Brasil\").agg(max(\"data\")).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|Recuperadosnovos|emAcompanhamentoNovos|\n",
      "+----------------+---------------------+\n",
      "|        17262646|              1065477|\n",
      "+----------------+---------------------+\n",
      "\n",
      "+--------------+----------+---------------+\n",
      "|casosAcumulado|casosNovos|incidenciaCasos|\n",
      "+--------------+----------+---------------+\n",
      "|      18855015|     62504|       8,972.29|\n",
      "+--------------+----------+---------------+\n",
      "\n",
      "+---------------+-----------+----------------+----------+\n",
      "|obitosAcumulado|obitosNovos|incidenciaObitos|letalidade|\n",
      "+---------------+-----------+----------------+----------+\n",
      "|         526892|       1780|          250.73|      2.79|\n",
      "+---------------+-----------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DadosPainel_1 = dados.filter(col(\"regiao\") == \"Brasil\").\\\n",
    "            filter(col(\"data\") == maxData).\\\n",
    "            select(\"Recuperadosnovos\",\"emAcompanhamentoNovos\") \n",
    "DadosPainel_2 = dados.filter(col(\"regiao\") == \"Brasil\").\\\n",
    "            filter(col(\"data\") == maxData).\\\n",
    "            select(\"casosAcumulado\",\"casosNovos\",\"incidenciaCasos\")\n",
    "DadosPainel_3 = dados.filter(col(\"regiao\") == \"Brasil\").\\\n",
    "            filter(col(\"data\") == maxData).\\\n",
    "            select(\"obitosAcumulado\",\"obitosNovos\",\"incidenciaObitos\",\"letalidade\")\n",
    "DadosPainel_1.show()\n",
    "DadosPainel_2.show()\n",
    "DadosPainel_3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Salvar a primeira visualização como tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DadosPainel_1.write.mode(\"overwrite\").saveAsTable('DadosPainel_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid19.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DadosPainel_2.write.parquet(\"/user/covid19/DadosPainel_2\", compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/covid19/DadosPainel_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_obitos = DadosPainel_3.select(\"obitosAcumulado\")\\\n",
    "                       .withColumnRenamed(\"obitosAcumulado\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_obitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-obitos\")\\\n",
    "              .save()\n",
    "\n",
    "kafka_novos_obitos = DadosPainel_3.select(\"obitosNovos\")\\\n",
    "                       .withColumnRenamed(\"obitosNovos\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_novos_obitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-NovosObitos\")\\\n",
    "              .save()\n",
    "\n",
    "kafka_incidenciaObitos = DadosPainel_3.select(\"incidenciaObitos\")\\\n",
    "                       .withColumnRenamed(\"incidenciaObitos\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_incidenciaObitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-incidenciaObitos\")\\\n",
    "              .save()\n",
    "\n",
    "kafka_letalidade = DadosPainel_3.select(\"letalidade\")\\\n",
    "                       .withColumnRenamed(\"letalidade\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_incidenciaObitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-letalidade\")\\\n",
    "              .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar a visualização do exercício 6 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o330.save.\n: java.lang.ClassNotFoundException: Failed to find data source: org.elasticsearch.spark.sql. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:244)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 12 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a66e24f01b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://desafiosemantix.kb.us-west1.gcp.cloud.es.io/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkafka_obitos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.elasticsearch.spark.sql\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.nodes.wan.only\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.port\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"9243\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.net.ssl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.nodes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.net.http.auth.user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"elastic\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.net.http.auth.pass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gPbGxozAcdl5rHBeXFdlnJdY\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.resource\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"covid19/_doc\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o330.save.\n: java.lang.ClassNotFoundException: Failed to find data source: org.elasticsearch.spark.sql. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:244)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 12 more\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://desafiosemantix.kb.us-west1.gcp.cloud.es.io/\"\n",
    "es = Elasticsearch.put_script()\n",
    "kafka_obitos.write \\\n",
    "           .format(\"org.elasticsearch.spark.sql\") \\\n",
    "           .option(\"es.nodes.wan.only\",\"true\") \\\n",
    "           .option(\"es.port\",\"9243\") \\\n",
    "           .option(\"es.net.ssl\",\"true\") \\\n",
    "           .option(\"es.nodes\", URL) \\\n",
    "           .option(\"es.net.http.auth.user\", \"elastic\") \\\n",
    "           .option(\"es.net.http.auth.pass\", \"gPbGxozAcdl5rHBeXFdlnJdY\") \\\n",
    "           .option(\"es.resource\", \"covid19/_doc\") \\\n",
    "           .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
