{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio Big Data Engineer - Semantix Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enviar os dados para o hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sudo cd spark/input\n",
    "\n",
    "sudo mkdir desafio\n",
    "\n",
    "sudo cd desafio\n",
    "\n",
    "sudo curl -O https://mobileapps.saude.gov.br/esus-vepi/files/unAFkcaNDeXajurGB7LChj8SgQYS2ptm/04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "sudo unrar x 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "sudo rm 04bd3419b22b9cc5c6efac2c6528100d_HIST_PAINEL_COVIDBR_06jul2021.rar\n",
    "\n",
    "docker exec -it namenode bash\n",
    "\n",
    "hdfs dfs -mkdir -p /user/eric/desafio\n",
    "\n",
    "hdfs dfs -put /input/desafio/* /user/eric/desafio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/eric/desafio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando Database beeline \n",
    "\n",
    "docker exec -it hive-server bash\n",
    "\n",
    "beeline -u jdbc:hive2://localhost:10000\n",
    "\n",
    "create database COVID19 comment 'Desafio Semantix';\n",
    "\n",
    "SET hive.exec.dynamic.partition = true;\n",
    "\n",
    "SET hive.exec.dynamic.partition.mode = nonstrict ;\n",
    "\n",
    "create table covid19Brutos(\n",
    "    regiao string,\n",
    "    estado string,\n",
    "    municipio string,    \n",
    "    coduf int,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data string,\n",
    "    semanaEpi int,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado string,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitana string)\n",
    "    ROW FORMAT DELIMITED FIELDS TERMINATED BY ';'\n",
    "    LINES TERMINATED BY '\\n'\n",
    "    STORED AS TEXTFILE;\n",
    "\n",
    "LOAD DATA INPATH '/user/eric/desafio/*.csv' OVERWRITE INTO TABLE covid19Brutos;\n",
    "\n",
    "create table covid19(\n",
    "    regiao string,\n",
    "    estado string,    \n",
    "    coduf int,\n",
    "    codmun int,\n",
    "    codRegiaoSaude int,\n",
    "    nomeRegiaoSaude string,\n",
    "    data string,\n",
    "    semanaEpi int,\n",
    "    populacaoTCU2019 int,\n",
    "    casosAcumulado int,\n",
    "    casosNovos int,\n",
    "    obitosAcumulado string,\n",
    "    obitosNovos int,\n",
    "    Recuperadosnovos int,\n",
    "    emAcompanhamentoNovos int,\n",
    "    interior_metropolitana string)\n",
    "    partitioned by (municipio string);\n",
    "\t\n",
    "insert overwrite table covid19 partition (municipio) select * from covid19Brutos;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criar as 3 vizualizações pelo Spark com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3cfd14edd8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='covid19', description='', locationUri='hdfs://namenode:8020/user/hive/warehouse/covid19.db'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Database de trabalho\n",
    "spark.catalog.setCurrentDatabase(\"covid19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se esta conectado no database correto\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dadospainel_1', database='covid19', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listando as telas\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------------+----------------+----------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|      data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|incidenciaCasos|incidenciaObitos|letalidade|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------------+----------------+----------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|           0.00|            0.00|      null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|           0.00|            0.00|      0.00|\n",
      "+------+------+---------+-----+------+--------------+---------------+----------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+---------------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: string (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: string (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: string (nullable = true)\n",
      " |-- incidenciaCasos: string (nullable = true)\n",
      " |-- incidenciaObitos: string (nullable = true)\n",
      " |-- letalidade: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2624943"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "estrutura_lista = [\n",
    "    \n",
    "    StructField(\"regiao\", StringType()),\n",
    "    StructField(\"estado\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"coduf\", StringType()),\n",
    "    StructField(\"codmun\", IntegerType()),\n",
    "    StructField(\"codRegiaoSaude\", IntegerType()),\n",
    "    StructField(\"nomeRegiaoSaude\", StringType()),\n",
    "    StructField(\"data\", StringType()),\n",
    "    StructField(\"semanaEpi\", IntegerType()),\n",
    "    StructField(\"populacaoTCU2019\", IntegerType()),\n",
    "    StructField(\"casosAcumulado\", IntegerType()),\n",
    "    StructField(\"casosNovos\", IntegerType()),\n",
    "    StructField(\"obitosAcumulado\", StringType()),\n",
    "    StructField(\"obitosNovos\", IntegerType()),\n",
    "    StructField(\"Recuperadosnovos\", IntegerType()),\n",
    "    StructField(\"emAcompanhamentoNovos\", IntegerType()),\n",
    "    StructField(\"interior/metropolitana\", StringType())\n",
    "]\n",
    "\n",
    "schema_names = StructType(estrutura_lista)\n",
    "\n",
    "dados = spark.read.csv(\"/user/eric/desafio/*.csv\", header='true',sep=';', schema = schema_names)\n",
    "dados = dados.withColumn(\"incidenciaCasos\", format_number(col(\"casosAcumulado\")/(col(\"populacaoTCU2019\")/100000),2))\n",
    "dados = dados.withColumn(\"incidenciaObitos\", format_number(col(\"obitosAcumulado\")/(col(\"populacaoTCU2019\")/100000),2))\n",
    "dados = dados.withColumn(\"letalidade\", format_number((col(\"obitosAcumulado\")/col(\"casosAcumulado\"))*100,2))\n",
    "\n",
    "dados.show(5)\n",
    "dados.printSchema()\n",
    "dados.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import timem\n",
    "ini = time.time()\n",
    "#dados.write.saveAsTable(\"covid19.DadosCovid19\",partitionBy=\"municipio\")\n",
    "dados.write.mode(\"overwrite\").partitionBy(\"municipio\").saveAsTable(\"covid19.DadosCovid\")\n",
    "fim = time.time()\n",
    "print (\"\\nTempo de Processamento: {0:4.2f} min\".format((fim-ini)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid19.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxData = dados.select(\"regiao\",\"data\").filter(col(\"regiao\") == \"Brasil\").agg(max(\"data\")).collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------------+\n",
      "|Recuperadosnovos|emAcompanhamentoNovos|\n",
      "+----------------+---------------------+\n",
      "|        17262646|              1065477|\n",
      "+----------------+---------------------+\n",
      "\n",
      "+--------------+----------+---------------+\n",
      "|casosAcumulado|casosNovos|incidenciaCasos|\n",
      "+--------------+----------+---------------+\n",
      "|      18855015|     62504|       8,972.29|\n",
      "+--------------+----------+---------------+\n",
      "\n",
      "+---------------+-----------+----------------+----------+\n",
      "|obitosAcumulado|obitosNovos|incidenciaObitos|letalidade|\n",
      "+---------------+-----------+----------------+----------+\n",
      "|         526892|       1780|          250.73|      2.79|\n",
      "+---------------+-----------+----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DadosPainel_1 = dados.filter(col(\"regiao\") == \"Brasil\").\\\n",
    "            filter(col(\"data\") == maxData).\\\n",
    "            select(\"Recuperadosnovos\",\"emAcompanhamentoNovos\") \n",
    "DadosPainel_2 = dados.filter(col(\"regiao\") == \"Brasil\").\\\n",
    "            filter(col(\"data\") == maxData).\\\n",
    "            select(\"casosAcumulado\",\"casosNovos\",\"incidenciaCasos\")\n",
    "DadosPainel_3 = dados.filter(col(\"regiao\") == \"Brasil\").\\\n",
    "            filter(col(\"data\") == maxData).\\\n",
    "            select(\"obitosAcumulado\",\"obitosNovos\",\"incidenciaObitos\",\"letalidade\")\n",
    "DadosPainel_1.show()\n",
    "DadosPainel_2.show()\n",
    "DadosPainel_3.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 Salvar a primeira visualização como tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DadosPainel_1.write.mode(\"overwrite\").saveAsTable('DadosPainel_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/covid19.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DadosPainel_2.write.parquet(\"/user/covid19/DadosPainel_2\", compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/covid19/DadosPainel_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar a terceira visualização em um tópico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_obitos = DadosPainel_3.select(\"obitosAcumulado\")\\\n",
    "                       .withColumnRenamed(\"obitosAcumulado\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_obitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-obitos\")\\\n",
    "              .save()\n",
    "\n",
    "kafka_novos_obitos = DadosPainel_3.select(\"obitosNovos\")\\\n",
    "                       .withColumnRenamed(\"obitosNovos\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_novos_obitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-NovosObitos\")\\\n",
    "              .save()\n",
    "\n",
    "kafka_incidenciaObitos = DadosPainel_3.select(\"incidenciaObitos\")\\\n",
    "                       .withColumnRenamed(\"incidenciaObitos\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_incidenciaObitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-incidenciaObitos\")\\\n",
    "              .save()\n",
    "\n",
    "kafka_letalidade = DadosPainel_3.select(\"letalidade\")\\\n",
    "                       .withColumnRenamed(\"letalidade\",\"value\")\\\n",
    "                       .withColumn(\"value\",col(\"value\").cast(StringType()))\n",
    "\n",
    "kafka_incidenciaObitos.write.format('kafka')\\\n",
    "              .option(\"kafka.bootstrap.servers\",\"kafka:9092\")\\\n",
    "              .option(\"topic\",\"topic-kafka-letalidade\")\\\n",
    "              .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar a visualização do exercício 6 em um tópico no Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'casosAcumulado': 18855015, 'casosNovos': 62504, 'incidenciaCasos': '8,972.29'}\n"
     ]
    }
   ],
   "source": [
    "doc = DadosPainel_2.collect()[0].asDict()\n",
    "doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'dadospainel2',\n",
       " '_type': '_doc',\n",
       " '_id': '1',\n",
       " '_version': 2,\n",
       " 'result': 'updated',\n",
       " '_shards': {'total': 2, 'successful': 2, 'failed': 0},\n",
       " '_seq_no': 1,\n",
       " '_primary_term': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = \"https://desafiosemantix.es.us-west1.gcp.cloud.es.io:9243/dadospainel2/_doc/1\"\n",
    "headers = {'Content-type': 'application/json; charset=utf8'}\n",
    "response = requests.post(URL, auth=('elastic', 'gPbGxozAcdl5rHBeXFdlnJdY'),json=doc,headers=headers)\n",
    "response.encoding\n",
    "response.text\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kafka_obitos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a66e24f01b74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://desafiosemantix.kb.us-west1.gcp.cloud.es.io/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mkafka_obitos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"org.elasticsearch.spark.sql\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.nodes.wan.only\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.port\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"9243\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.net.ssl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.nodes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.net.http.auth.user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"elastic\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.net.http.auth.pass\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gPbGxozAcdl5rHBeXFdlnJdY\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"es.resource\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"covid19/_doc\"\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kafka_obitos' is not defined"
     ]
    }
   ],
   "source": [
    "URL = \"https://desafiosemantix.kb.us-west1.gcp.cloud.es.io/\"\n",
    "kafka_obitos.write \\\n",
    "           .format(\"org.elasticsearch.spark.sql\") \\\n",
    "           .option(\"es.nodes.wan.only\",\"true\") \\\n",
    "           .option(\"es.port\",\"9243\") \\\n",
    "           .option(\"es.net.ssl\",\"true\") \\\n",
    "           .option(\"es.nodes\", URL) \\\n",
    "           .option(\"es.net.http.auth.user\", \"elastic\") \\\n",
    "           .option(\"es.net.http.auth.pass\", \"gPbGxozAcdl5rHBeXFdlnJdY\") \\\n",
    "           .option(\"es.resource\", \"covid19/_doc\") \\\n",
    "           .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
